# -*- coding: utf-8 -*-
"""BDA-RedditDSPosts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13eLV_c-fv1jD2llxlkfuy_3zr9cZ5MHQ
"""

from google.colab import drive
drive.mount('/content/drive')
#drive.flush_and_unmount()

# üì¶ Step 1: Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import numpy as np
import re
import string
import unicodedata

all_data = pd.read_csv('/content/drive/MyDrive/reddit_data_processed_time.csv')

all_data['created_date'] = all_data['created_date'].astype('datetime64[ns]')
all_data.head()

all_data.info()

all_data.describe().style.background_gradient(cmap = 'inferno')

"""# Exploratory Data Analysis(EDA)"""

plt.title('Subreddits (sum of posts)', color = 'white', size = 15)
sub_redd = all_data.groupby(all_data["subreddit"]).created_date.count().sort_values(ascending = False)
sub_redd.plot(kind = "bar", edgecolor = '#ff4501', color = '#ff4501')
plt.xlabel('')
plt.show()

plt.title('Posts Distribution', color = 'black', size = 15)
year_d = all_data.groupby(all_data["created_date"].dt.year).created_date.count()
year_d.plot(kind = "bar", edgecolor = '#ff4501', color = '#ff4501')
plt.xlabel('Posts per year')
plt.xticks(rotation = 45)
plt.show()

plt.title('Posts Distribution', color = 'white', size = 15)
month_d = all_data.groupby(all_data["created_date"].dt.month).created_date.count()
month_d.plot(kind = "bar", edgecolor = '#ff4501', color = '#ff4501')
plt.xlabel('Year activity (month)')
plt.xticks(rotation = 'horizontal')
plt.show()

plt.title('Posts Distribution', color = 'white', size = 15)
day_d = all_data.groupby(all_data["created_date"].dt.day).created_date.count()
day_d.plot(kind = "bar", edgecolor = '#ff4501', color = '#ff4501')
plt.xlabel('Month activity (day)')
plt.show()

plt.title('Posts Distribution', color = 'white', size = 15)
day_d = all_data.groupby(all_data["created_date"].dt.day).created_date.count()
day_d.plot(kind = "bar", edgecolor = '#ff4501', color = '#ff4501')
plt.xlabel('Month activity (day)')
plt.show()

def text_cleaner(text):
    """
    Function for clearing text data from unnecessary characters.
    """
    text = text.lower()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    text = re.sub('\[.*?\]', ' ', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', ' ', text)
    text = re.sub('\r', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

tqdm.pandas()
title_cleaned = all_data['title'].progress_apply(lambda x: text_cleaner(x))
title_cleaned

all_data['title_cleaned'] = title_cleaned

from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

all_data['sentiment_compound'] = title_cleaned.apply(lambda x: sia.polarity_scores(x)['compound'])
all_data['sentiment_label'] = all_data['sentiment_compound'].apply(
    lambda score: 'positive' if score > 0.05 else ('negative' if score < -0.05 else 'neutral')
)

all_data[['title_cleaned', 'sentiment_compound', 'sentiment_label']].head(10)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.countplot(data=all_data, x='sentiment_label', palette='coolwarm')
plt.title("Sentiment Distribution of Reddit Post Titles")
plt.show()

# Make sure your datetime column is parsed and set
all_data['created_date'] = pd.to_datetime(all_data['created_timestamp'], unit='s')

# Group by month and get mean sentiment
sentiment_over_time = all_data.groupby(all_data['created_date'].dt.to_period("M"))['sentiment_compound'].mean()

# Plot it
sentiment_over_time.plot(figsize=(12, 6), title="Average Sentiment Over Time")
plt.xlabel("Time")
plt.ylabel("Average Sentiment Score")
plt.show()

sns.boxplot(data=all_data, x='sentiment_label', y='score', hue='sentiment_label',palette='Set2')
plt.title("Post Score by Sentiment")
plt.yscale('log')
plt.show()

# Top 5 most positive posts
print("üåû Top Positive Posts:")
print(all_data.sort_values(by='sentiment_compound', ascending=False)[['title', 'sentiment_compound']].head(5))

# Top 5 most negative posts
print("\nüåßÔ∏è Top Negative Posts:")
print(all_data.sort_values(by='sentiment_compound', ascending=True)[['title', 'sentiment_compound']].head(5))

"""# LDA"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Vectorize cleaned titles
vectorizer = CountVectorizer(max_features=1000, stop_words='english')

X = vectorizer.fit_transform(title_cleaned.fillna(""))

# Train LDA model
lda = LatentDirichletAllocation(n_components=5, random_state=42, learning_method='batch', verbose=1)
lda.fit(X)

!pip install wordcloud

# Print topics
for idx, topic in enumerate(lda.components_):
    top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]
    print(f"üßµ Topic {idx + 1}:", top_words)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Topic keywords and weights
n_topics = 5
n_top_words = 30

# Create word clouds for each topic
for idx, topic in enumerate(lda.components_):
    print(f"üåÄ Topic {idx + 1}")

    # Get top word indices and corresponding weights
    top_indices = topic.argsort()[-n_top_words:]
    top_words = {vectorizer.get_feature_names_out()[i]: topic[i] for i in top_indices}

    # Generate and show word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_words)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Topic {idx + 1}", fontsize=16)
    plt.show()

"""# Similarity Analysis"""

from cuml.feature_extraction.text import TfidfVectorizer
from cuml.neighbors import NearestNeighbors
import cupy as cp

# Use the cleaned title + selftext as the input
all_data['post'] = title_cleaned + " " + all_data['post'].fillna("")

# TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english', max_features=1000)
X = tfidf.fit_transform(all_data['post'].astype(str))

# Fit Nearest Neighbors on GPU
knn = NearestNeighbors(n_neighbors=6, metric='cosine')  # 6 = 1 original + 5 similar
knn.fit(X)

import cudf
all_data = cudf.DataFrame.from_pandas(all_data)

def get_similar_to_title(input_title, top_n=10):
    from cudf import Series  # make sure this is imported

    # Vectorize the cleaned input title
    input_vector = tfidf.transform(Series([text_cleaner(input_title)]))

    # Find similar posts
    distances, indices = knn.kneighbors(input_vector, n_neighbors=top_n)
    similarity_scores = 1 - distances.flatten()

    # Get top similar posts
    results = all_data.iloc[indices[0]][['id', 'title', 'score', 'subreddit']].copy()
    results['similarity'] = similarity_scores.round(3)

    return results.sort_values(by='similarity', ascending=False).reset_index(drop=True)

from IPython.display import display

input_title = input("üîç Enter your post title to find similar Reddit posts: ")
similar_posts = get_similar_to_title(input_title)
display(similar_posts)

"""# Virality Predictor"""

# Using a higher percentile threshold
viral_threshold = all_data['score'].quantile(0.75)
all_data['is_viral'] = all_data['score'] > viral_threshold

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

X_text = tfidf.fit_transform(all_data['title_cleaned'].fillna(""))

# Numeric features
X_numeric = all_data[['sentiment_compound', 'num_comments', 'subreddit_subscribers']].copy()
X_numeric.fillna(0, inplace=True)

import cupy as cp
from cupyx.scipy.sparse import csr_matrix as cupy_csr
from scipy.sparse import csr_matrix, hstack

# If X_text is a CuPy sparse matrix:
X_text_scipy = csr_matrix(cp.asnumpy(X_text.toarray()))

# Now convert X_numeric to sparse
X_numeric_sparse = csr_matrix(X_numeric.values)

# Now safely stack
X_combined = hstack([X_text_scipy, X_numeric_sparse])

y = all_data['is_viral']

# 7Ô∏è‚É£ Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# 8Ô∏è‚É£ Train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

def predict_virality(user_title, num_comments=0, sentiment_score=0, subreddit_subscribers=0):
    # Clean the input title
    cleaned_title = text_cleaner(user_title)

    # Vectorize text ‚Äî CuML expects a Pandas Series
    X_title = tfidf.transform(pd.Series([cleaned_title]))
    X_title_scipy = csr_matrix(cp.asnumpy(X_title.toarray()))

    # Create numeric feature array
    numeric_input = np.array([[sentiment_score, num_comments, subreddit_subscribers]])
    X_numeric_input = csr_matrix(numeric_input)

    # Combine
    X_input_combined = hstack([X_title_scipy, X_numeric_input])

    # Predict
    prob_viral = model.predict_proba(X_input_combined)[0][1]  # probability of class 1 (viral)

    print(f"üì¢ Virality likelihood for this post: **{prob_viral * 100:.2f}%**")
    return prob_viral

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Wedge

def show_virality_gauge(score):
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set_xlim(-1.2, 1.2)
    ax.set_ylim(-0.2, 1.2)
    ax.axis('off')

    # Color zones
    zones = [
        (0, 54, 'red'),
        (54, 108, 'orange'),
        (108, 144, 'yellowgreen'),
        (144, 180, 'green')
    ]

    for start, end, color in zones:
        wedge = Wedge(center=(0, 0), r=1, theta1=start, theta2=end, facecolor=color, alpha=0.6)
        ax.add_patch(wedge)

    # Needle angle
    angle = 180 * score
    x = np.cos(np.radians(180 - angle))
    y = np.sin(np.radians(180 - angle))

    # Needle
    ax.arrow(0, 0, x * 0.8, y * 0.8, width=0.015, head_width=0.05, head_length=0.1, fc='black', ec='black')

    # Center circle
    ax.add_patch(plt.Circle((0, 0), 0.05, color='black'))

    # Display percentage
    ax.text(0, -0.15, f"Virality: {int(score * 100)}%", ha='center', fontsize=16, fontweight='bold')
    plt.title("Virality Prediction Gauge", fontsize=18)
    plt.show()

score = predict_virality("I want to learn data analytics, statistics and machine learning.", num_comments=20, sentiment_score=0.7, subreddit_subscribers=50000)
show_virality_gauge(score)

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
def predict_virality_from_title(user_title):
    # Clean title
    cleaned_title = text_cleaner(user_title)

    # Compute sentiment
    sentiment_score = analyzer.polarity_scores(cleaned_title)['compound']

    # Use dataset means for other features
    avg_num_comments = all_data['num_comments'].mean()
    avg_subreddit_subs = all_data['subreddit_subscribers'].mean()

    # Ensure the input is a pandas Series
    X_title = tfidf.transform(pd.Series([cleaned_title]))
    X_title_scipy = csr_matrix(cp.asnumpy(X_title.toarray()))

    # Numeric features
    numeric_input = np.array([[sentiment_score, avg_num_comments, avg_subreddit_subs]])
    X_numeric_input = csr_matrix(numeric_input)

    # Combine
    X_input_combined = hstack([X_title_scipy, X_numeric_input])

    # Predict
    prob_viral = model.predict_proba(X_input_combined)[0][1]

    print(f"üì¢ Virality likelihood for this post: **{prob_viral * 100:.2f}%**")
    return prob_viral

# Get user input and call the predictor
user_title = input("üìù Enter your Reddit post title: ")
predict_virality_from_title(user_title)

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

"""# Download data"""

all_data.to_csv("reddit_data_processed_time.csv", index=False)

from google.colab import files
files.download("reddit_data_processed_time.csv")

import sqlite3
conn = sqlite3.connect("reddit_data.db")  # This creates reddit_data.db
all_data.to_sql("reddit_posts", conn, if_exists="replace", index=False)

from google.colab import files
files.download("reddit_data.db")

import joblib

joblib.dump(lda, "lda_model.pkl")
files.download("lda_model.pkl")

joblib.dump(vectorizer, "vectorizer.pkl")
files.download("vectorizer.pkl")

"""# Topic Trend Predictor"""

from google.colab import files
uploaded = files.upload()

import joblib

lda_model = joblib.load("lda_model.pkl")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Vectorize cleaned titles
vectorizer = CountVectorizer(max_features=1000, stop_words='english')

doc_term_matrix = vectorizer.fit_transform(all_data['title_cleaned'].fillna(""))

# 2Ô∏è‚É£ Get topic distributions for each document
topic_distributions = lda_model.transform(doc_term_matrix)

# 3Ô∏è‚É£ Assign dominant topic to each post
all_data['dominant_topic'] = topic_distributions.argmax(axis=1)

for idx, topic in enumerate(lda_model.components_):
    top_words_idx = topic.argsort()[-10:][::-1]
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]
    print(f"Topic {idx}: {', '.join(top_words)}")

topic_labels = {
    0: "Data Science Careers",
    1: "AI + PyTorch/TensorFlow Usage",
    2: "Regression Help & Dataset Questions",
    3: "ML/AI/Deep Learning + Interviews",
    4: "Neural Networks, Statistics, Advice"
}

all_data.columns

all_data['week'] = pd.to_datetime(all_data['created_date']).dt.to_period('W')

# Optional: Make sure all 5 topics are present
for topic in range(5):
    if topic not in weekly_topic_counts.columns:
        weekly_topic_counts[topic] = 0
weekly_topic_counts = weekly_topic_counts.sort_index(axis=1)
# Display first few rows of the weekly topic trend table
weekly_topic_counts.head()

import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Convert PeriodIndex to string for plotting
weekly_topic_counts.index = weekly_topic_counts.index.astype(str)

# Plot + forecast for each topic
for topic in weekly_topic_counts.columns:
    series = weekly_topic_counts[topic]

    if series.sum() < 10:
        print(f"Skipping Topic {topic} ‚Äî not enough data")
        continue

    model = ExponentialSmoothing(series, trend='add', seasonal=None, damped_trend=True)
    fit = model.fit()
    forecast = fit.forecast(10)

    # Plot
    plt.figure(figsize=(10, 4))
    plt.plot(series.index, series.values, label=f"Topic {topic} - Actual", marker='o')
    plt.plot(forecast.index.astype(str), forecast.values, label="Forecast", linestyle='--', marker='x')
    plt.title(f"Topic {topic} Trend Forecast")
    plt.xlabel("Week")
    plt.ylabel("Post Count")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def map_title_to_topic(title, lda_model, vectorizer):
    cleaned = text_cleaner(title)
    bow_vector = vectorizer.transform([cleaned])
    topic_dist = lda_model.transform(bow_vector)
    dominant_topic = int(np.argmax(topic_dist))
    return dominant_topic, topic_dist

def is_emerging_topic(topic, weekly_df, forecast_steps=2, threshold=0.05):
    topic = int(topic)
    series = weekly_df[topic]
    model = ExponentialSmoothing(series, trend='add', seasonal=None)
    fit = model.fit()
    forecast = fit.forecast(forecast_steps)

    trend_now = series[-2:].mean()
    trend_next = forecast[:2].mean()

    if trend_now == 0:
        return trend_next > 0
    change = (trend_next - trend_now) / trend_now
    return change > threshold

import random
import calendar
from datetime import timedelta

def get_best_post_days(topic, weekly_df, steps=6):
    series = weekly_df[topic]
    model = ExponentialSmoothing(series, trend='add', seasonal=None)
    fit = model.fit()
    forecast = fit.forecast(steps)

    best_weeks = forecast.sort_values(ascending=False).head(2).index

    best_days = []
    for period in best_weeks:
        # Convert Period to datetime (start of the week)
        start_date = period.start_time

        # Pick a random day in the week
        random_day = start_date + timedelta(days=random.randint(0, 6))
        best_days.append(calendar.day_name[random_day.weekday()])

    return best_days

title = "not working in excel"

weekly_topic_counts = all_data.groupby(['week', 'dominant_topic']).size().unstack(fill_value=0)
# 1Ô∏è‚É£ Map title to topic
topic, dist = map_title_to_topic(title, lda_model, vectorizer)
print(f"üéØ Dominant Topic: {topic} - {topic_labels[topic]}")


# 2Ô∏è‚É£ Check if it's emerging
if is_emerging_topic(topic, weekly_topic_counts):
    print("üìà This is an *emerging* topic!")
else:
    print("üìâ Topic might be stable or declining.")

# 3Ô∏è‚É£ Get best weeks to post
top_weeks = get_best_post_days(topic, weekly_topic_counts)
print("‚úÖ Best days to post:")
for week in top_weeks:
    print(f"üóìÔ∏è {week}")

import numpy as np
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import calendar
import warnings

# Optional: Suppress warnings from statsmodels if they appear
warnings.filterwarnings("ignore", category=UserWarning, module='statsmodels')

# --- Helper Functions ---

def map_title_to_topic(title, lda_model, vectorizer):
    """Maps a raw title string to its dominant topic index."""
    # Simple cleaning (consistency matters - match LDA training cleaning)
    cleaned = title.lower()
    try:
        # Use the *fitted* vectorizer to transform the new title
        bow_vector = vectorizer.transform([cleaned])
        topic_dist = lda_model.transform(bow_vector)
        dominant_topic = int(np.argmax(topic_dist))
        return dominant_topic, topic_dist
    except Exception as e:
        print(f"Error mapping title to topic: {e}")
        return None, None

def is_emerging_topic(topic, weekly_df, forecast_steps=4, min_data_points=10):
    """Checks if a topic's volume is trending upwards based on recent history and forecast."""
    if weekly_df is None or topic not in weekly_df.columns:
        print(f"Warning: Topic {topic} not found in weekly data.")
        return False # Cannot determine if data is missing

    series = weekly_df[topic]

    # Check if there's enough data to model reliably
    if series.sum() < min_data_points or len(series.dropna()) < min_data_points:
         # print(f"Topic {topic}: Not enough historical data ({series.sum()} posts) for reliable trend analysis.")
         return False # Not enough data to call it emerging

    try:
        # Use damped trend for stability, allow additive trend
        model = ExponentialSmoothing(series, trend='add', seasonal=None, damped_trend=False)
        fit = model.fit()
        forecast = fit.forecast(forecast_steps)

        # Compare recent trend vs forecasted trend
        if len(series) >= 2 and len(forecast) >=2:
            trend_now = series.iloc[-2:].mean()
            trend_next = forecast.iloc[:2].mean()
            # Add a small threshold to avoid flagging minor fluctuations as "emerging"
            return trend_next > trend_now # e.g., forecast must be > 5% higher than recent avg
        elif len(series) >= 1 and len(forecast) >= 1:
             # Fallback if only one point available
             return forecast.iloc[0] > series.iloc[-1]
        else:
            return False # Not enough points to compare

    except Exception as e:
        # Log error or handle gracefully
        print(f"Warning: Could not perform trend analysis for topic {topic}: {e}")
        return False # Default to not emerging if model fails

def get_best_post_days(topic, historical_data):
    """
    Identifies the historically best day(s) of the week to post for a given topic.

    Args:
        topic (int): The dominant topic index.
        historical_data (pd.DataFrame): The main dataframe containing 'dominant_topic'
                                        and 'day_of_week' columns.

    Returns:
        list: A list of day names (e.g., ['Monday', 'Thursday']) representing the
              historically most active days for this topic. Returns empty list on error.
    """
    # Ensure required columns exist in the dataframe passed
    if historical_data is None or not all(col in historical_data.columns for col in ['dominant_topic', 'day_of_week']):
        print("Error: Historical data is missing required columns ('dominant_topic', 'day_of_week').")
        return []
    try:
        # Filter data for the specific topic
        topic_data = historical_data[historical_data['dominant_topic'] == topic]

        if topic_data.empty:
            print(f"Warning: No historical data found for topic {topic} to determine best posting days.")
            # Example fallback: return overall most popular days
            # overall_days = historical_data['day_of_week'].value_counts().nlargest(1).index.tolist()
            # return [calendar.day_name[day_idx] for day_idx in overall_days]
            return ["Data unavailable"]

        # Count posts per day of the week for this topic
        day_counts = topic_data['day_of_week'].value_counts()

        # Get the day index(es) with the highest count
        max_count = day_counts.max()
        best_day_indices = day_counts[day_counts == max_count].index.tolist()

        # Convert day indices (0-6) to day names and sort them Mon-Sun
        best_days = sorted([calendar.day_name[day_idx] for day_idx in best_day_indices], key=lambda x: list(calendar.day_name).index(x))

        return best_days

    except Exception as e:
        print(f"Error analyzing best posting days for topic {topic}: {e}")
        return []

# --- Function Call Example ---
# Assuming 'all_data', 'lda_model', 'vectorizer', 'weekly_topic_counts',
# and 'topic_labels' are already defined and loaded in your environment.

print("\n--- Running Topic Analysis Example ---")
title = "is agi the next big thing?" # Your example title

# 1Ô∏è‚É£ Map title to topic
topic, dist = map_title_to_topic(title, lda_model, vectorizer) # Use your loaded model/vectorizer

if topic is not None:
    # Ensure topic_labels is defined and has the topic key
    if 'topic_labels' in locals() and topic in topic_labels:
        topic_name = topic_labels[topic]
    else:
        topic_name = f"Unknown Topic {topic}" # Fallback if labels missing

    print(f"\nInput Title: '{title}'")
    print(f"üéØ Dominant Topic: {topic} - {topic_name}")

    # 2Ô∏è‚É£ Check if it's emerging
    # Ensure weekly_topic_counts is defined
    if 'weekly_topic_counts' in locals():
        if is_emerging_topic(topic, weekly_topic_counts): # Use your loaded counts
            print("üìà Trend: This topic appears to be *emerging* or growing.")
        else:
            print("üìâ Trend: This topic appears *stable or declining*.")
    else:
        print("‚ö†Ô∏è Trend analysis skipped: 'weekly_topic_counts' not found.")


    # 3Ô∏è‚É£ Get best historical posting days (using the revised function)
    # Ensure all_data is defined and has required columns
    if 'all_data' in locals():
        best_days = get_best_post_days(topic, all_data) # Use your loaded data
        if best_days and best_days != ["Data unavailable"]:
             print(f"‚úÖ Historically Best Day(s) to Post for Topic {topic}: {', '.join(best_days)}")
        elif best_days == ["Data unavailable"]:
             print(f"‚ö†Ô∏è Could not determine specific best posting days for Topic {topic} (insufficient data).")
        else:
             print(f"‚ùå Could not determine best posting days for Topic {topic}.")
    else:
        print("‚ö†Ô∏è Best day analysis skipped: 'all_data' not found.")


else:
    print(f"Could not determine topic for title: '{title}'")

all_data['day_of_week'] = all_data['created_date'].dt.dayofweek # 0=Monday, 6=Sunday

all_data['created_date'] = pd.to_datetime(all_data['created_date'], errors='coerce')

all_data.columns

"""# Controversy Predictor

Using a binary classifier
"""

epsilon=1e+6
all_data['controversy_ratio'] = all_data['num_comments'] / (abs(all_data['score']) + 1 + epsilon)

# --- Step 1: Create the Binary is_controversial Label ---

# Define your thresholds (TUNE THESE BASED ON YOUR DATA EXPLORATION!)
# Using quantiles for ratio and a fixed min for comments is often a good start.
try:
    # Calculate threshold based on the 90th percentile of the ratio
    # Filter out potential outliers before calculating quantile if needed, e.g., ratio < 100
    valid_ratios = all_data.loc[all_data['controversy_ratio'].notna() & np.isfinite(all_data['controversy_ratio']), 'controversy_ratio']
    threshold_ratio = valid_ratios.quantile(0.90) # Label top 10% ratio as potentially controversial

    min_comments = 20 # Example: Require at least 20 comments

    print(f"Using controversy ratio threshold: > {threshold_ratio:.4f} (90th percentile)")
    print(f"Using minimum comments threshold: >= {min_comments}")

    # Initialize the column to 0 (not controversial)
    all_data['is_controversial'] = 0

    # Apply the thresholds to set controversial posts to 1
    # Ensure num_comments exists
    if 'num_comments' in all_data.columns:
        all_data.loc[
            (all_data['controversy_ratio'] > threshold_ratio) &
            (all_data['num_comments'] >= min_comments),
            'is_controversial'
        ] = 1
    else:
        print("‚ùå Error: 'num_comments' column not found. Cannot apply min comments threshold.")
        # Decide how to proceed - maybe only use ratio? Or stop.
        # For now, we'll proceed, but the label might be less accurate.
        all_data.loc[
            (all_data['controversy_ratio'] > threshold_ratio),
            'is_controversial'
        ] = 1
        print("‚ö†Ô∏è Warning: Labeling based only on ratio threshold due to missing 'num_comments'.")


    # Check the distribution of the new label
    print("\nDistribution of 'is_controversial' label:")
    print(all_data['is_controversial'].value_counts(normalize=True))
    print(all_data['is_controversial'].value_counts())

except KeyError as e:
    print(f"‚ùå Error: Missing column required for labeling: {e}")
except Exception as e:
    print(f"‚ùå An error occurred during label creation: {e}")

# Display a sample to verify
# print("\nSample data with new label:")
# print(all_data[['score', 'num_comments', 'controversy_ratio', 'is_controversial']].head(10))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
# --- Use RandomForestClassifier ---
from sklearn.ensemble import RandomForestClassifier
from scipy.sparse import hstack, csr_matrix
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib

# --- Step 1: Ensure 'is_controversial' column exists ---
# Run the code from the previous step again if needed to create/verify the column
# print(all_data['is_controversial'].value_counts(normalize=True))


# --- Step 2: Define the Classifier Training Function (using RandomForest) ---

def train_controversy_classifier_rf(data, text_col='title_cleaned', target_col='is_controversial'):
    """
    Trains a RandomForest model to predict post controversy (binary label).
    """
    print(f"\n--- Starting Controversy RandomForest Classifier Training ---")

    # --- Feature Definition & Validation (Same as before) ---
    numeric_features = ['sentiment_compound', 'subreddit_subscribers']
    categorical_feature = 'dominant_topic'
    required_cols = [text_col] + numeric_features + [categorical_feature, target_col]
    if not all(col in data.columns for col in required_cols):
        missing = [col for col in required_cols if col not in data.columns]
        print(f"‚ùå Error: Data is missing required columns: {missing}")
        return None, None, None
    df = data.copy()
    df[text_col] = df[text_col].fillna("")
    for col in numeric_features:
        median_val = df[col].median()
        df[col] = df[col].fillna(median_val)
    df[categorical_feature] = df[categorical_feature].fillna(-1).astype(int)
    df[target_col] = df[target_col].astype(int)
    y = df[target_col]
    if y.nunique() < 2:
        print(f"‚ùå Error: Target variable '{target_col}' has less than 2 unique values.")
        return None, None, None
    print(f"Target distribution:\n{y.value_counts(normalize=True)}")

    X_text_series = df[text_col]
    X_numeric_df = df[numeric_features]
    X_cat_df = df[[categorical_feature]]

    # --- Train-Test Split (Same as before) ---
    print("Splitting data into training and testing sets (80/20)...")
    try:
        X_text_train, X_text_test, \
        X_numeric_train, X_numeric_test, \
        X_cat_train, X_cat_test, \
        y_train, y_test = train_test_split(
            X_text_series, X_numeric_df, X_cat_df, y,
            test_size=0.2, random_state=42, stratify=y
        )
        print("Splitting complete.")
    except ValueError as e:
         print(f"‚ùå Error during train-test split: {e}")
         return None, None, None

    # --- Feature Processing (Same as before) ---
    print("Processing features (TF-IDF, OneHotEncoding)...")
    tfidf_vectorizer = TfidfVectorizer(max_features=1500, stop_words='english', min_df=5, ngram_range=(1,2))
    X_text_train_tfidf = tfidf_vectorizer.fit_transform(X_text_train)
    X_text_test_tfidf = tfidf_vectorizer.transform(X_text_test)
    print(f"TF-IDF fitted with {len(tfidf_vectorizer.vocabulary_)} features.")

    X_numeric_train_sparse = csr_matrix(X_numeric_train.values)
    X_numeric_test_sparse = csr_matrix(X_numeric_test.values)

    ohe_topic = OneHotEncoder(handle_unknown='ignore', sparse_output=True)
    X_cat_train_ohe = ohe_topic.fit_transform(X_cat_train)
    X_cat_test_ohe = ohe_topic.transform(X_cat_test)
    print(f"Topic OneHotEncoder fitted.")

    X_train_combined = hstack([X_text_train_tfidf, X_numeric_train_sparse, X_cat_train_ohe])
    X_test_combined = hstack([X_text_test_tfidf, X_numeric_test_sparse, X_cat_test_ohe])
    print(f"Combined training features shape: {X_train_combined.shape}")

    # --- Model Training (Using RandomForestClassifier) ---
    print("Training RandomForest Classifier model...")
    # Adjust parameters: n_estimators, max_depth are important
    # n_jobs=-1 uses all available CPU cores
    model = RandomForestClassifier(
        n_estimators=100,           # Number of trees (100 is a reasonable start)
        class_weight='balanced',    # Still important for imbalance
        random_state=42,
        n_jobs=-1,                  # Use all cores
        max_depth=15,               # Limit tree depth to prevent overfitting (TUNABLE)
        min_samples_leaf=5          # Require at least 5 samples per leaf (TUNABLE)
    )
    try:
        model.fit(X_train_combined, y_train)
        print("‚úÖ Model training complete.")
    except Exception as e:
        print(f"‚ùå Error during model training: {e}")
        return None, None, None

    # --- Evaluation (Same as before) ---
    print("\n--- Evaluating RandomForest model on test set ---")
    try:
        y_pred = model.predict(X_test_combined)
        y_proba = model.predict_proba(X_test_combined)[:, 1]

        print("Classification Report:")
        print(classification_report(y_test, y_pred, target_names=['Not Controversial (0)', 'Controversial (1)'], zero_division=0)) # Added zero_division

        print("Confusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"[[TN={cm[0,0]} FP={cm[0,1]}]\n [FN={cm[1,0]} TP={cm[1,1]}]]")

        try:
            auc = roc_auc_score(y_test, y_proba)
            print(f"\nAUC Score: {auc:.4f}")
        except ValueError as auc_err:
             print(f"\nCould not calculate AUC: {auc_err}")

    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error during evaluation: {e}")

    print("\n--- Returning trained model and transformers ---")
    return model, tfidf_vectorizer, ohe_topic


# --- Step 3: Call the RandomForest Classification Function ---

# Ensure 'is_controversial' column exists first
if 'is_controversial' in all_data.columns:
    # Train the RandomForest classification model
    rf_controversy_model, rf_controversy_tfidf, rf_controversy_ohe = train_controversy_classifier_rf(all_data, target_col='is_controversial')

    if rf_controversy_model and rf_controversy_tfidf and rf_controversy_ohe:
        print("\n‚úÖ RandomForest Classification Training successful! Objects returned.")
        # You could save these, but we plan to drop the feature, so maybe not necessary
        # joblib.dump(rf_controversy_model, 'rf_controversy_model.pkl')
        # joblib.dump(rf_controversy_tfidf, 'rf_controversy_tfidf.pkl')
        # joblib.dump(rf_controversy_ohe, 'rf_controversy_ohe.pkl')
else:
    print("‚ùå Error: 'is_controversial' column not found or not created. Cannot train classifier.")
    rf_controversy_model, rf_controversy_tfidf, rf_controversy_ohe = None, None, None

import numpy as np
import pandas as pd
from scipy.sparse import hstack, csr_matrix
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.utils.validation import check_is_fitted
from sklearn.exceptions import NotFittedError

# --- Assume these are already loaded in your Colab environment ---
# rf_controversy_model: Your trained RandomForestClassifier
# rf_controversy_tfidf: The fitted TfidfVectorizer used for the RF model
# rf_controversy_ohe: The fitted OneHotEncoder for topics used for the RF model
# lda_model: Your fitted LDA model
# vectorizer_lda: The fitted CountVectorizer used for the LDA model
# all_data: Your original DataFrame (used to get average subscriber count)
# ----------------------------------------------------------------

# --- Helper function from before (ensure it's defined) ---
def is_vectorizer_fitted(vectorizer):
    try:
        check_is_fitted(vectorizer)
        return True
    except NotFittedError:
        return False
    except Exception: # Catch other potential errors like None object
        return False

def map_title_to_topic(title, lda_model, vectorizer):
    if lda_model is None or vectorizer is None: return None, None
    cleaned = title.lower()
    try:
        if not is_vectorizer_fitted(vectorizer):
             print("‚ö†Ô∏è LDA Vectorizer not fitted in map_title_to_topic.")
             return None, None
        bow_vector = vectorizer.transform([cleaned])
        topic_dist = lda_model.transform(bow_vector)
        dominant_topic = int(np.argmax(topic_dist))
        return dominant_topic, topic_dist
    except Exception as e:
        print(f"‚ö†Ô∏è Error mapping title to topic: {e}")
        return None, None
# ----------------------------------------------------------------

# --- Initialize Sentiment Analyzer ---
try:
    sia = SentimentIntensityAnalyzer()
    print("Sentiment Analyzer initialized.")
except Exception as e:
    print(f"‚ùå Error initializing Sentiment Analyzer: {e}. Make sure VADER lexicon is downloaded.")
    sia = None

# --- Calculate Average Subscriber Count (Do this ONCE) ---
try:
    if 'subreddit_subscribers' in all_data.columns:
        # Calculate median instead of mean, less sensitive to outliers
        avg_subreddit_subscribers = all_data['subreddit_subscribers'].median()
        print(f"Calculated median subreddit subscribers: {avg_subreddit_subscribers:.0f}")
    else:
        print("‚ö†Ô∏è 'subreddit_subscribers' column not found, using 0 as default.")
        avg_subreddit_subscribers = 0
except NameError:
     print("‚ö†Ô∏è 'all_data' DataFrame not found, using 0 for avg subscribers.")
     avg_subreddit_subscribers = 0
except Exception as e:
     print(f"‚ö†Ô∏è Error calculating avg subscribers: {e}. Using 0.")
     avg_subreddit_subscribers = 0


# --- Prediction Function ---
# --- Prediction Function (Modified Part) ---
def predict_controversy_probability(
    input_title,
    model,
    tfidf_vec,
    ohe_topic_enc,
    lda_model_map,
    vectorizer_lda_map,
    sentiment_analyzer,
    default_sub_subscribers,
    # --- ADD parameter for the expected topic column name ---
    topic_column_name='dominant_topic'
    ):
    """
    Predicts the probability of a given title being controversial.
    """
    # --- Input Checks (remain the same) ---
    if not all([model, tfidf_vec, ohe_topic_enc, lda_model_map, vectorizer_lda_map, sentiment_analyzer]):
        print("‚ùå Error: One or more required models/vectorizers/analyzers are missing.")
        return None
    if not isinstance(input_title, str) or not input_title:
        print("‚ùå Error: Invalid input title.")
        return None
    if not is_vectorizer_fitted(tfidf_vec) or not is_vectorizer_fitted(vectorizer_lda_map):
         print("‚ùå Error: One or more required vectorizers are not fitted.")
         return None

    try:
        # --- 1. Preprocess Title (remains the same) ---
        cleaned_title = input_title.lower()

        # --- 2. Extract Features ---
        # a) TF-IDF (remains the same)
        X_text_tfidf = tfidf_vec.transform([cleaned_title])

        # b) Sentiment (remains the same)
        sentiment_compound = sentiment_analyzer.polarity_scores(cleaned_title)['compound']

        # c) Topic (Get topic index)
        dominant_topic, _ = map_title_to_topic(cleaned_title, lda_model_map, vectorizer_lda_map)
        if dominant_topic is None:
            print("‚ùå Error: Could not determine dominant topic for the title.")
            return None

        # --- *** MODIFICATION HERE *** ---
        # Create a DataFrame with the correct column name before OHE transform
        X_cat_topic_df = pd.DataFrame([[dominant_topic]], columns=[topic_column_name])
        X_cat_ohe = ohe_topic_enc.transform(X_cat_topic_df)
        # --- *** END MODIFICATION *** ---


        # d) Numeric Features (remains the same)
        X_numeric = csr_matrix([[sentiment_compound, default_sub_subscribers]])

        # --- 3. Combine Features (remains the same) ---
        X_combined = hstack([X_text_tfidf, X_numeric, X_cat_ohe])

        # --- 4. Predict Probability (remains the same) ---
        probabilities = model.predict_proba(X_combined)
        controversy_prob = probabilities[0][1]

        return controversy_prob

    except Exception as e:
        print(f"‚ùå An error occurred during prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

# --- Example Usage
if 'rf_controversy_model' in locals() and rf_controversy_model is not None:
    test_title = "SQL is dead, everyone should use NoSQL now."

    # --- Make sure 'vectorizer' here is correct name for your FITTED LDA vectorizer ---
    # --- It was 'vectorizer_lda' in previous steps. Double-check this variable name! ---
    lda_vec_to_use = vectorizer_lda if 'vectorizer_lda' in locals() else (vectorizer if 'vectorizer' in locals() else None)

    if lda_vec_to_use is not None:
        predicted_prob = predict_controversy_probability(
            input_title=test_title,
            model=rf_controversy_model,
            tfidf_vec=rf_controversy_tfidf,
            ohe_topic_enc=rf_controversy_ohe,
            lda_model_map=lda_model,
            vectorizer_lda_map=lda_vec_to_use, # Use the checked variable
            sentiment_analyzer=sia,
            default_sub_subscribers=avg_subreddit_subscribers
            # topic_column_name='dominant_topic'
        )

        if predicted_prob is not None:
            print(f"\nInput Title: '{test_title}'")
            print(f"Predicted Controversy Probability: {predicted_prob:.4f} ({predicted_prob*100:.2f}%)")
        else:
            print(f"\nCould not predict controversy probability for title: '{test_title}'")
    else:
        print("\n‚ùå Cannot predict: Fitted LDA Vectorizer ('vectorizer_lda' or 'vectorizer') not found.")
else:
    print("\n‚ùå Controversy RandomForest model ('rf_controversy_model') not found.")

